{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implementation Using N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-gram Dictionaries extracted and saved.\n",
      "Filtered n-gram dictionary generated.\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Word",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Importance Score",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "033b8240-c774-4290-a544-5081e9ce1036",
       "rows": [
        [
         "0",
         "want die",
         "43103.20604710764"
        ],
        [
         "1",
         "want kill",
         "24208.852730866613"
        ],
        [
         "2",
         "suicidal thought",
         "21820.07792655197"
        ],
        [
         "3",
         "ve try",
         "20665.049534414222"
        ],
        [
         "4",
         "want end",
         "19782.049869965256"
        ],
        [
         "5",
         "want live",
         "18992.76627558624"
        ],
        [
         "6",
         "end life",
         "18285.13610750684"
        ],
        [
         "7",
         "commit redflag",
         "17959.903719223505"
        ],
        [
         "8",
         "friend family",
         "13876.058312790221"
        ],
        [
         "9",
         "know want",
         "13288.564233799321"
        ],
        [
         "10",
         "know anymore",
         "13142.115739797122"
        ],
        [
         "11",
         "month ago",
         "13094.405618015999"
        ],
        [
         "12",
         "know feel",
         "12945.350743493484"
        ],
        [
         "13",
         "want feel",
         "12927.989814491422"
        ],
        [
         "14",
         "live life",
         "12666.618769914592"
        ],
        [
         "15",
         "past year",
         "12539.83824054047"
        ],
        [
         "16",
         "feel way",
         "12319.663000975672"
        ],
        [
         "17",
         "try kill",
         "12027.382738843899"
        ],
        [
         "18",
         "life want",
         "11109.682685544389"
        ],
        [
         "19",
         "life feel",
         "11044.110210402307"
        ],
        [
         "20",
         "try hard",
         "11020.916975840672"
        ],
        [
         "21",
         "family friend",
         "10999.289257792043"
        ],
        [
         "22",
         "want want",
         "10939.991650677188"
        ],
        [
         "23",
         "ve feel",
         "10833.799438451257"
        ],
        [
         "24",
         "try help",
         "10783.769667137263"
        ],
        [
         "25",
         "self harm",
         "10525.067345129315"
        ],
        [
         "26",
         "people care",
         "10355.66738218663"
        ],
        [
         "27",
         "know know",
         "9997.669658345565"
        ],
        [
         "28",
         "think kill",
         "9908.308628433422"
        ],
        [
         "29",
         "anymore want",
         "9896.1355482044"
        ],
        [
         "30",
         "week ago",
         "9884.563201613231"
        ],
        [
         "31",
         "need talk",
         "9864.60539400983"
        ],
        [
         "32",
         "want anymore",
         "9843.69208726988"
        ],
        [
         "33",
         "piece shit",
         "9568.268704230031"
        ],
        [
         "34",
         "look forward",
         "9481.844680168339"
        ],
        [
         "35",
         "like want",
         "9432.55262347747"
        ],
        [
         "36",
         "die want",
         "9297.233941141505"
        ],
        [
         "37",
         "want stop",
         "9289.893580253169"
        ],
        [
         "38",
         "time feel",
         "9260.353931643389"
        ],
        [
         "39",
         "live anymore",
         "9117.862437298892"
        ],
        [
         "40",
         "like shit",
         "8809.33316782395"
        ],
        [
         "41",
         "people know",
         "8795.217366780545"
        ],
        [
         "42",
         "want help",
         "8784.900226495789"
        ],
        [
         "43",
         "want hurt",
         "8746.531273254288"
        ],
        [
         "44",
         "know long",
         "8740.133044039036"
        ],
        [
         "45",
         "ve think",
         "8666.975353103513"
        ],
        [
         "46",
         "think redflag",
         "8628.257993772746"
        ],
        [
         "47",
         "year ve",
         "8624.77884075282"
        ],
        [
         "48",
         "live like",
         "8423.218929368024"
        ],
        [
         "49",
         "ve lose",
         "8403.264780987822"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 100
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Importance Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>want die</td>\n",
       "      <td>43103.206047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>want kill</td>\n",
       "      <td>24208.852731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>suicidal thought</td>\n",
       "      <td>21820.077927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ve try</td>\n",
       "      <td>20665.049534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>want end</td>\n",
       "      <td>19782.049870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>people think</td>\n",
       "      <td>1.642011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>like know</td>\n",
       "      <td>1.545719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>people like</td>\n",
       "      <td>1.344910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>look like</td>\n",
       "      <td>0.819315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>wan na</td>\n",
       "      <td>0.708014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Word  Importance Score\n",
       "0           want die      43103.206047\n",
       "1          want kill      24208.852731\n",
       "2   suicidal thought      21820.077927\n",
       "3             ve try      20665.049534\n",
       "4           want end      19782.049870\n",
       "..               ...               ...\n",
       "95      people think          1.642011\n",
       "96         like know          1.545719\n",
       "97       people like          1.344910\n",
       "98         look like          0.819315\n",
       "99            wan na          0.708014\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"data/preprocessing/mental_health_preprocessed.csv\")\n",
    "\n",
    "# remove rows that have null values in the \"text_lemma\" column\n",
    "df = df.dropna(subset=['text_lemma'])\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=42)\n",
    "\n",
    "\n",
    "# Define TF-IDF vectorizer for n-grams (bigrams and trigrams)\n",
    "vectorizer = TfidfVectorizer(ngram_range=(2, 3), stop_words=\"english\", max_features=100)\n",
    "X_train_tfidf = vectorizer.fit_transform(train_df[\"text_lemma\"])\n",
    "X_test_tfidf = vectorizer.transform(test_df[\"text_lemma\"])\n",
    "\n",
    "# Extract dictionary from each corpus using TF-IDF n-grams\n",
    "dictionary_0 = set(vectorizer.get_feature_names_out())  # Full vocabulary of n-grams\n",
    "\n",
    "# Save the dictionary for future use\n",
    "with open(\"data/dictionaries/dictionary_tfidf_ngrams.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(dictionary_0))\n",
    "\n",
    "print(\"N-gram Dictionaries extracted and saved.\")\n",
    "\n",
    "# Separate texts based on label\n",
    "texts_train_0 = train_df[train_df[\"label\"] == 0][\"text_lemma\"].tolist()\n",
    "texts_train_1 = train_df[train_df[\"label\"] == 1][\"text_lemma\"].tolist()\n",
    "\n",
    "# Apply TF-IDF separately for each group using n-grams\n",
    "vectorizer_0 = TfidfVectorizer(ngram_range=(2, 3), stop_words=\"english\", max_features=100)\n",
    "tfidf_0 = vectorizer_0.fit_transform(texts_train_0)\n",
    "ngrams_0 = vectorizer_0.get_feature_names_out()\n",
    "scores_0 = np.asarray(tfidf_0.mean(axis=0)).flatten()\n",
    "\n",
    "vectorizer_1 = TfidfVectorizer(ngram_range=(2, 3), stop_words=\"english\", max_features=100)\n",
    "tfidf_1 = vectorizer_1.fit_transform(texts_train_1)\n",
    "ngrams_1 = vectorizer_1.get_feature_names_out()\n",
    "scores_1 = np.asarray(tfidf_1.mean(axis=0)).flatten()\n",
    "\n",
    "# Create DataFrames with extracted n-grams and scores\n",
    "df_tfidf_0 = pd.DataFrame({\"ngram\": ngrams_0, \"score\": scores_0}).sort_values(by=\"score\", ascending=False)\n",
    "df_tfidf_1 = pd.DataFrame({\"ngram\": ngrams_1, \"score\": scores_1}).sort_values(by=\"score\", ascending=False)\n",
    "\n",
    "# Compute relative importance of n-grams (frequency ratio between classes)\n",
    "ngram_weights = {}\n",
    "for ngram in set(ngrams_1):\n",
    "    freq_1 = df_tfidf_1[df_tfidf_1[\"ngram\"] == ngram][\"score\"].values[0] if ngram in df_tfidf_1[\"ngram\"].values else 0\n",
    "    freq_0 = df_tfidf_0[df_tfidf_0[\"ngram\"] == ngram][\"score\"].values[0] if ngram in df_tfidf_0[\"ngram\"].values else 0\n",
    "    ngram_weights[ngram] = freq_1 / (freq_0 + 1e-6)  # Avoid division by zero\n",
    "\n",
    "# Sort n-grams by importance (higher values = more relevant to mentally ill class)\n",
    "filtered_ngrams = sorted(ngram_weights.items(), key=lambda x: x[1], reverse=True)[:100]  # Keep top 100 n-grams\n",
    "\n",
    "# Save filtered n-gram dictionary\n",
    "with open(\"data/dictionaries/filtered_dictionary_ngrams.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join([ngram for ngram, score in filtered_ngrams]))\n",
    "\n",
    "print(\"Filtered n-gram dictionary generated.\")\n",
    "\n",
    "df_filtered_ngrams = pd.DataFrame(filtered_ngrams, columns=[\"Word\", \"Importance Score\"])\n",
    "df_filtered_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-gram dictionaries extracted and saved.\n",
      "Logistic Regression Accuracy: 0.5055\n",
      "Training and test sets created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Extract dictionary from each corpus using TF-IDF n-grams\n",
    "dictionary_0 = set(df_tfidf_0[\"ngram\"])  # Healthy n-grams\n",
    "dictionary_1 = set(df_tfidf_1[\"ngram\"])  # Mentally ill n-grams\n",
    "\n",
    "# Save the dictionaries for future use\n",
    "with open(\"data/dictionaries/dictionary_healthy_ngrams.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(dictionary_0))\n",
    "\n",
    "with open(\"data/dictionaries/dictionary_mentally_ill_ngrams.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(dictionary_1))\n",
    "\n",
    "print(\"N-gram dictionaries extracted and saved.\")\n",
    "\n",
    "# Validate using Logistic Regression\n",
    "vectorizer_filtered = TfidfVectorizer(vocabulary=[ngram for ngram, _ in filtered_ngrams])\n",
    "X_train_filtered = vectorizer_filtered.fit_transform(train_df[\"text_lemma\"])\n",
    "X_test_filtered = vectorizer_filtered.transform(test_df[\"text_lemma\"])\n",
    "\n",
    "# Train logistic regression model\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train_filtered, train_df[\"label\"])\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_filtered)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(test_df[\"label\"], y_pred)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Display confirmation\n",
    "print(\"Training and test sets created successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
